{"title":"Agent Coder","markdown":{"yaml":{"title":"Agent Coder","description":"This blog explores the implementation of an code developing agent framework called Agent Coder using LangGraph, a specialized SDK for building robust and reliable LLM-based agents. We detail the architecture and methods to handle complex coding tasks autonomously. The blog post provides practical insights into integrating LangGraph’s capabilities to enhance agentic systems for developers.","author":"Oliver Pfante","date":"2024-12-15","categories":["agentic","code"],"image":"agent_coder.webp","toc":true,"jupyter":"python3","execute":{"enabled":false},"bibliography":"../references.bibtex"},"headingText":"**Key Features of AgentCoder**","containsRefs":false,"markdown":"\n\n\n\nAgentCoder is a multi-agent framework designed to enhance code generation by leveraging three distinct roles to streamline the coding and validation process. This implementation is directly derived from the paper *\"AgentCoder: Multi-Agent Code Generation with Effective Testing and Self-Optimization\"*[@huang2024agentcodermultiagentbasedcodegeneration], with some minor modifications to the prompts to optimize its functionality within LangGraph.\n\n1. **Three Roles**: \n   - **Programmer**: Responsible for generating code based on requirements.\n   - **Test Designer**: Generates comprehensive, independent test cases without being influenced by the program code. The test cases cover basic ones, edge cases, and large scale test cases, testing for performance.\n   - **Test Executor**: A program (not an LLM persona) that runs the tests and provides feedback for further code refinement.\n2. **Independent Test Creation**: Ensures objectivity and enhances the reliability of generated tests.\n3. **Iterative Improvement**: Refines code through multiple iterations based on test feedback until all tests pass.\n\n### **Notebook Objectives**\n- Illustrate the implementation of the AgentCoder framework using LangGraph.\n- Showcase the use of **Checkpointer (Memory)** in LangGraph to access the state of an executed graph.\n- Direct derivation of the implementation from the research paper, with minor modifications to the prompts.\n\n## Setup\n\n## Prompts\n\nWe start with writing down the relevant prompts. They are copy-pasted from the appendix of the paper.\n\n## Templates\n\nWe require the output of the Test Designer to be structured. To achieve this, we define a Pydantic class.\n\n## Runnables\n\nThe LangChain [Runnable interface](https://python.langchain.com/docs/concepts/runnables/) provides a flexible and composable abstraction for building and chaining operations, including models, prompts, and tools. The Runnables will then be executed in the LangGraph nodes.\n\nThe Programmer’s prompt template includes both the instruction and a one-shot example. Later, the prompt will include the chat-history as context.\n\nAdditionally, we crate a [Trustcall Runnable](https://github.com/hinthornw/trustcall) `tests`. Trustcall is a Python package that enhances the generation of JSON schemas from LLM responses.\n\n## States\n\nWe define a separate Input state to keep the unchanged input separated from the state channels which will be updated: the test cases, and the code.\n\n## Nodes\n\nThe nodes of the Graph invoke the previously defined Runnables.\n\n## Conditional Edges\n\n## Build the Graph\n\nWe add [Short-term Memory](https://langchain-ai.github.io/langgraph/concepts/memory/) to the Graph because we want to access the Graph's state after it has been executed. Recording the state updates by enabling check points also helps with debugging.\n\n## Run the Agent\n\nExecute the graph in streaming mode and render the messages. We use the ‘values’ streaming mode, which retains the entire chat history at each step. To focus on updates, only the most recent message is displayed. As input we pick an example of the [HumanEval](https://github.com/openai/human-eval)[@chen2021codex] test set.\n\nLast, we access the final implementation and run it against the HumanEval test cases. \n","srcMarkdownNoYaml":"\n\n\n\nAgentCoder is a multi-agent framework designed to enhance code generation by leveraging three distinct roles to streamline the coding and validation process. This implementation is directly derived from the paper *\"AgentCoder: Multi-Agent Code Generation with Effective Testing and Self-Optimization\"*[@huang2024agentcodermultiagentbasedcodegeneration], with some minor modifications to the prompts to optimize its functionality within LangGraph.\n\n### **Key Features of AgentCoder**\n1. **Three Roles**: \n   - **Programmer**: Responsible for generating code based on requirements.\n   - **Test Designer**: Generates comprehensive, independent test cases without being influenced by the program code. The test cases cover basic ones, edge cases, and large scale test cases, testing for performance.\n   - **Test Executor**: A program (not an LLM persona) that runs the tests and provides feedback for further code refinement.\n2. **Independent Test Creation**: Ensures objectivity and enhances the reliability of generated tests.\n3. **Iterative Improvement**: Refines code through multiple iterations based on test feedback until all tests pass.\n\n### **Notebook Objectives**\n- Illustrate the implementation of the AgentCoder framework using LangGraph.\n- Showcase the use of **Checkpointer (Memory)** in LangGraph to access the state of an executed graph.\n- Direct derivation of the implementation from the research paper, with minor modifications to the prompts.\n\n## Setup\n\n## Prompts\n\nWe start with writing down the relevant prompts. They are copy-pasted from the appendix of the paper.\n\n## Templates\n\nWe require the output of the Test Designer to be structured. To achieve this, we define a Pydantic class.\n\n## Runnables\n\nThe LangChain [Runnable interface](https://python.langchain.com/docs/concepts/runnables/) provides a flexible and composable abstraction for building and chaining operations, including models, prompts, and tools. The Runnables will then be executed in the LangGraph nodes.\n\nThe Programmer’s prompt template includes both the instruction and a one-shot example. Later, the prompt will include the chat-history as context.\n\nAdditionally, we crate a [Trustcall Runnable](https://github.com/hinthornw/trustcall) `tests`. Trustcall is a Python package that enhances the generation of JSON schemas from LLM responses.\n\n## States\n\nWe define a separate Input state to keep the unchanged input separated from the state channels which will be updated: the test cases, and the code.\n\n## Nodes\n\nThe nodes of the Graph invoke the previously defined Runnables.\n\n## Conditional Edges\n\n## Build the Graph\n\nWe add [Short-term Memory](https://langchain-ai.github.io/langgraph/concepts/memory/) to the Graph because we want to access the Graph's state after it has been executed. Recording the state updates by enabling check points also helps with debugging.\n\n## Run the Agent\n\nExecute the graph in streaming mode and render the messages. We use the ‘values’ streaming mode, which retains the entire chat history at each step. To focus on updates, only the most recent message is displayed. As input we pick an example of the [HumanEval](https://github.com/openai/human-eval)[@chen2021codex] test set.\n\nLast, we access the final implementation and run it against the HumanEval test cases. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"01_agent_coder.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"lux","title-block-banner":true,"title":"Agent Coder","description":"This blog explores the implementation of an code developing agent framework called Agent Coder using LangGraph, a specialized SDK for building robust and reliable LLM-based agents. We detail the architecture and methods to handle complex coding tasks autonomously. The blog post provides practical insights into integrating LangGraph’s capabilities to enhance agentic systems for developers.","author":"Oliver Pfante","date":"2024-12-15","categories":["agentic","code"],"image":"agent_coder.webp","jupyter":"python3","bibliography":["../references.bibtex"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
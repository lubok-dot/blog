{"title":"The CRITIC","markdown":{"yaml":{"title":"The CRITIC","description":"This blog evaluates a CRITIC agent implemented using the LangGraph SDK, focusing on solving arithmetic and ambiguous questions. It utilizes the GSM8K and AmbigQA datasets to test mathematical reasoning and ambiguity resolution, respectively. The agent integrates computational and search tools for evaluating the correctness of the LLM generated answers.","author":"Oliver Pfante","date":"2024-11-23","categories":["agentic","code"],"image":"critic.webp","toc":true,"jupyter":"python3","execute":{"enabled":false},"bibliography":"../references.bibtex"},"headingText":"Key Features","containsRefs":false,"markdown":"\n\n\nThe [CRITIC framework](https://arxiv.org/abs/2305.11738)[@Critic] is designed to enhance the reliability of Large Language Models (LLMs) by incorporating a human-like verify-then-correct approach. It leverages external tools to validate and iteratively refine outputs generated by LLMs. This framework addresses common issues such as hallucinations, faulty reasoning, and toxicity in LLM-generated content. In the subsequent notebook we focus on Question-Answering and Mathematical Reasoning. \n\n1. **Verification:** External tools critique the LLM’s generated output for accuracy and consistency.\n2. **Correction:** The LLM uses feedback from the critiques to improve its output.\n3. **Iterative Refinement:** The process repeats until the output satisfies predefined criteria.\n\n### Applications\n\n- **Free-form Question Answering:** Improves truthfulness and factual accuracy by querying a search engine.\n- **Mathematical Reasoning:** Solves Mathematical problems by translating them into Python code which is then executed in a confined environment. \n\n## Setup\n\n## Prompts\n\nThe attribute $p$ of the state of the Critic Agent.\n\n## Runnables\n\nCRITIC integrates external tools to perform verification tasks:\n\n- **[Tavily Search](https://python.langchain.com/docs/integrations/tools/tavily_search/#tool-features)(TavilySearchResults):**\n   - Validates factual correctness.\n   - Retrieves and processes web-based evidence.\n\n- **[Riza Code Interpreter](https://python.langchain.com/docs/integrations/tools/riza/#related) (ExecPython):**\n   - Executes Python code to support the LLM's mathematical reasoning.\n   - Provides feedback on execution results and errors.\n\n\n**Key Points**\n\n  - Tavily Search: Configured to retrieve detailed search results, supporting robust verification for question answering. \n  - RIZA Code Intperpreter: Executes Python programs to assess correctness and provide feedback on mathematical reasoning tasks which are translated into python code. \n  - TOOL_MAP: Organizes tools for seamless execution: we can call each tool by its name.\n\nNext, we instantiate a Large Language Model.\n\n## States\n\nWe implement the Critic Agent as outlined in Algorithm 1 of the [Critic Paper](http://arxiv.org/abs/2305.11738)[@Critic]. The agent is implemented using [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/), a framework that defines agents as state machines represented as graphs.\n\n- **Nodes**: Each node in the graph is a function that takes the state as input and returns an updated version of the state as output.\n- **Edges**: Each edge connects nodes and dictates transitions between them.\n\n### State of the Critic Agent\n\nThe state of the Critic Agent contains the following attributes:\n\n- **$p$**: The system prompt.\n- **$x$**: The input message.\n- **$y$**: The entire message history, stored as a list $[y_0, y_1, ..., y_i]$ instead of just the latest output message $y_i$. This approach aligns with examples such as those in the Appendix of [@Critic].\n- **$c$**: The responses of external tools (the critiques).\n\n### Modifications to Align with Practical Implementation\n\nCompared to the pseudo-code in Algorithm 1, we modify the state of the Critic Agent as follows:\n\n1. **Message History ($y$)**: Instead of storing only the latest output $y_i$, we maintain the full history of messages $[y_0, y_1, ..., y_i]$. This change ensures richer context availability during the iterative process. For instance, in tasks like TriviaQA (Appendix of [@Critic]), maintaining full history facilitates better verification and correction.\n2. **Iteration Tracking**: We store the number of iterations the agent has gone through the critic stage. This attribute ensures that the process halts after a user-defined maximum number of iterations, preventing infinite loops.\n\n### Rationale for Full Message History\n\nThe choice to store the entire message history ($y$) instead of just the latest output ($y_i$) is demonstrated in practical applications, such as the TriviaQA example in the Critic Paper. In this example:\n\n- The question requires determining *which innovation for the car was developed by Prince Henry of Prussia in 1911*. To answer:\n  - The Critic Agent first queries information about Prince Henry of Prussia to identify his contributions and historical context.\n  - Subsequently, the agent queries information on innovations he developed in 1911 to pinpoint the car-related invention.\n- Retaining the full history allows the second query to reference outputs from the first query, ensuring consistency and accuracy in the iterative reasoning process.\n\nWithout full message history, the Critic Agent might lose context from earlier tool outputs, leading to incomplete or inconsistent answers. By maintaining all intermediate steps and critiques, the agent ensures comprehensive and context-aware responses. This design choice is critical for multi-step, tool-dependent tasks where earlier results inform later reasoning. \n\n## Nodes\n\nThe Critic Agent's graph has two nodes, `assistant` and `tools`, which collaborate to iteratively answer the query.\n\n- **`assistant` Node**:\n  - **Input**: The initial question and the `Observation` (result of a tool invocation).\n  - **Processing**: \n    - The `assistant` generates a `Thought` by reasoning about whether the current information is sufficient to answer the question.\n    - If more information is needed, the `assistant` generates an `Action`, specifying a tool call and its arguments (e.g., search query for Tavily Search or Python code for Riza Code Interpreter).\n  - **Output**: A `Thought` (decision) and, if needed, an `Action` (tool call).\n\n- **`tools` Node**:\n  - **Input**: The `Action` generated by the `assistant`.\n  - **Execution**: Invokes the specified tool with the provided arguments and retrieves the result.\n  - **Output**: An `Observation`, which is passed back to the `assistant`.\n\n#### Example (TriviaQA):\nFor the question *“Which innovation for the car was developed by Prince Henry of Prussia in 1911?”*:\n\n1. The `assistant` node receives the query and decides to search for information about Henry of Prussia, generating an `Action` (search query).\n2. The `tools` node executes the search and returns the result (`Observation`) to the `assistant`.\n3. The `assistant` evaluates the observation, determines that more information is needed, and generates another `Action` to search for innovations by Prince Henry.\n4. The `tools` node performs the second search, and the result allows the `assistant` to conclude the answer.\n\nThis iterative process continues until the `assistant` determines the answer or a termination condition is met (e.g., maximum iterations).\n\n#### Interaction and Graph Dynamics:\n\nThe Critic Agent operates as a state machine:\n\n- **Nodes**: Functions that process and transform the agent's state.\n- **Edges**: Transitions between nodes based on the agent’s decisions (`Thought` and `Action`).\n\n#### State Attributes as Independent Channels\n\nIn LangGraph, each attribute of the state, such as `p`, `x`, `c`, and `y`, functions as an independent **channel** that can be updated individually. \n\n- **Partial Updates**: Functions operating on the state (nodes) do not need to return a complete state object. Instead, they can selectively update one or more attributes (channels) while leaving the others unchanged.\n- **State Compilation**: During the graph compilation process, LangGraph ensures that all node functions output a complete state object. It merges the unchanged attributes with the updated ones, creating a seamless transition between nodes.\n- **Example**: In the `assistant` node, the function only updates the `y` attribute of the state. Similarly, the `tools` node updates `c` and `num_iterations`. LangGraph handles the integration of these updates into the full state object.\n\nThis feature simplifies node implementation and enhances modularity, as each node can focus solely on the attributes it directly affects. LangGraph ensures the consistency and integrity of the full state during execution. \n\n## Conditional Edges\n\nThe `tool_call` function defines a conditional edge in the Critic Agent's graph. This edge determines whether the agent invokes a tool for additional observations or concludes its execution. \n\n- **Conditions**:\n  1. **No Pending Tool Calls**:\n     - If the most recent output (`state.y[-1]`) does not contain any tool calls, the agent assumes the assistant node has provided a satisfactory result validated by the Critic.\n     - The agent transitions to the `END` node, returning the result as the final output.\n  2. **Maximum Iterations Reached**:\n     - If the number of iterations (`state.num_iterations`) equals or exceeds the predefined limit (`MAX_NUM_ITERATIONS`), the agent forcefully terminates its process to prevent infinite loops.\n\n- **Code Logic**:\n  ```python\n  return END if (not state.y[-1].tool_calls) or state.num_iterations >= MAX_NUM_ITERATIONS else \"tools\"\n\n## Build the Graph\n\nLangGraph models applications as **state machines**, where the system transitions between defined states based on inputs and conditions. A state machine is a mathematical abstraction used to represent a system with a finite number of states, transitions, and actions. For more details, refer to the [Wikipedia article on finite-state machines](https://en.wikipedia.org/wiki/Finite-state_machine).\n\nThis cell defines the graph for the Critic Agent, representing its workflow as a state machine.\n\n#### Key Components:\n\n1. **State Class**:\n   - The `CriticState` class defines the state of the agent, storing attributes such as the system prompt (`p`), input (`x`), message history (`y`), tool responses (`c`), and the number of iterations (`num_iterations`).\n\n2. **Nodes**:\n   - Nodes are **functions mapping a state onto a new state**. Each node takes a `CriticState` as input and returns an updated `CriticState` after performing a specific operation:\n     - **`assistant`**: Processes the current state to generate a `Thought` (decision) and an `Action` (e.g., a tool call).\n     - **`tools`**: Executes the specified tool action and updates the state with the resulting observation.\n\n3. **Edges**:\n   - Edges are **functions on states** mapping them onto nodes. They dictate the control flow between them:\n     - **`START → assistant`**: The graph starts at the `assistant` node.\n     - **`assistant → tools` (Conditional Edge)**:\n       - The `tool_call` function determines whether to transition to the `tools` node or terminate the process based on whether additional tool calls are needed or the iteration limit is reached.\n     - **`tools → assistant`**: After executing a tool action, control flows back to the `assistant` node for further reasoning.\n\n4. **Graph Compilation**:\n   - The graph is constructed using LangGraph's `StateGraph` and compiled with `builder.compile()` to produce the executable state machine (`critic`).\n\n#### Workflow:\nThe Critic Agent alternates between reasoning (`assistant`) and tool execution (`tools`) until:\n\n1. It produces a satisfactory result validated by the Critic, or\n2. It reaches the maximum iteration limit (`MAX_NUM_ITERATIONS`).\n\nThis design ensures iterative improvement of the response while enforcing efficient termination when appropriate.\n\n## Run the Agent\n\nWe evaluate the model on 5 examples each from GSM8K[@cobbe2021gsm8k] and AmbigNQ[@min2020ambigqa] [@kwiatkowski2019natural]. The [GSM8K](https://github.com/openai/grade-school-math?utm_source=chatgpt.com) (Grade School Math 8K) dataset is a collection of 8.5K high-quality, linguistically diverse grade school math word problems. It is designed to evaluate the arithmetic reasoning capabilities of large language models (LLMs). The [AmbigNQ](https://github.com/shmsw25/AmbigQA) dataset is a collection of 14k questions derived from the NQ-open benchmark, specifically designed to explore ambiguity in open-domain question answering. For further details, refer to the [data utilities documentation](./utils/01_data.html).\n\n### Define Configuration\n\nWhen executing our agent, we can specify runtime parameters using the `config` argument in the `invoke` or `batch` methods. This argument accepts a [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) instance to control the agent’s behavior. In this example, we set a recursion limit of 5 to prevent the agent from calling the same node more than five times. Additionally, we limit the agent to a maximum of five concurrent calls to the language model (LLM) during batch operations. While the recursion limit serves to cap the number of iterations, it’s important to note that exceeding this limit will cause the agent to exit with an error, rather than completing its execution.\n\n### GSM8K\n\n#### Observations from the Output\n\n1. The CRITIC agent successfully translated mathematical problems into Python code and executed them using the Riza Code Interpreter tool.  \n2. The implementation achieved success on the first attempt, with no need for code revisions or re-writing.  \n\n### AmbigQA\n\n#### Observations from the Output\n\n1. The CRITIC agent successfully translated QA problems into search requests handled with TavilySearch.\n2. The implementation achieved success on the first attempt, with no need for further searches.  \n\n","srcMarkdownNoYaml":"\n\n\nThe [CRITIC framework](https://arxiv.org/abs/2305.11738)[@Critic] is designed to enhance the reliability of Large Language Models (LLMs) by incorporating a human-like verify-then-correct approach. It leverages external tools to validate and iteratively refine outputs generated by LLMs. This framework addresses common issues such as hallucinations, faulty reasoning, and toxicity in LLM-generated content. In the subsequent notebook we focus on Question-Answering and Mathematical Reasoning. \n\n### Key Features\n1. **Verification:** External tools critique the LLM’s generated output for accuracy and consistency.\n2. **Correction:** The LLM uses feedback from the critiques to improve its output.\n3. **Iterative Refinement:** The process repeats until the output satisfies predefined criteria.\n\n### Applications\n\n- **Free-form Question Answering:** Improves truthfulness and factual accuracy by querying a search engine.\n- **Mathematical Reasoning:** Solves Mathematical problems by translating them into Python code which is then executed in a confined environment. \n\n## Setup\n\n## Prompts\n\nThe attribute $p$ of the state of the Critic Agent.\n\n## Runnables\n\nCRITIC integrates external tools to perform verification tasks:\n\n- **[Tavily Search](https://python.langchain.com/docs/integrations/tools/tavily_search/#tool-features)(TavilySearchResults):**\n   - Validates factual correctness.\n   - Retrieves and processes web-based evidence.\n\n- **[Riza Code Interpreter](https://python.langchain.com/docs/integrations/tools/riza/#related) (ExecPython):**\n   - Executes Python code to support the LLM's mathematical reasoning.\n   - Provides feedback on execution results and errors.\n\n\n**Key Points**\n\n  - Tavily Search: Configured to retrieve detailed search results, supporting robust verification for question answering. \n  - RIZA Code Intperpreter: Executes Python programs to assess correctness and provide feedback on mathematical reasoning tasks which are translated into python code. \n  - TOOL_MAP: Organizes tools for seamless execution: we can call each tool by its name.\n\nNext, we instantiate a Large Language Model.\n\n## States\n\nWe implement the Critic Agent as outlined in Algorithm 1 of the [Critic Paper](http://arxiv.org/abs/2305.11738)[@Critic]. The agent is implemented using [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/), a framework that defines agents as state machines represented as graphs.\n\n- **Nodes**: Each node in the graph is a function that takes the state as input and returns an updated version of the state as output.\n- **Edges**: Each edge connects nodes and dictates transitions between them.\n\n### State of the Critic Agent\n\nThe state of the Critic Agent contains the following attributes:\n\n- **$p$**: The system prompt.\n- **$x$**: The input message.\n- **$y$**: The entire message history, stored as a list $[y_0, y_1, ..., y_i]$ instead of just the latest output message $y_i$. This approach aligns with examples such as those in the Appendix of [@Critic].\n- **$c$**: The responses of external tools (the critiques).\n\n### Modifications to Align with Practical Implementation\n\nCompared to the pseudo-code in Algorithm 1, we modify the state of the Critic Agent as follows:\n\n1. **Message History ($y$)**: Instead of storing only the latest output $y_i$, we maintain the full history of messages $[y_0, y_1, ..., y_i]$. This change ensures richer context availability during the iterative process. For instance, in tasks like TriviaQA (Appendix of [@Critic]), maintaining full history facilitates better verification and correction.\n2. **Iteration Tracking**: We store the number of iterations the agent has gone through the critic stage. This attribute ensures that the process halts after a user-defined maximum number of iterations, preventing infinite loops.\n\n### Rationale for Full Message History\n\nThe choice to store the entire message history ($y$) instead of just the latest output ($y_i$) is demonstrated in practical applications, such as the TriviaQA example in the Critic Paper. In this example:\n\n- The question requires determining *which innovation for the car was developed by Prince Henry of Prussia in 1911*. To answer:\n  - The Critic Agent first queries information about Prince Henry of Prussia to identify his contributions and historical context.\n  - Subsequently, the agent queries information on innovations he developed in 1911 to pinpoint the car-related invention.\n- Retaining the full history allows the second query to reference outputs from the first query, ensuring consistency and accuracy in the iterative reasoning process.\n\nWithout full message history, the Critic Agent might lose context from earlier tool outputs, leading to incomplete or inconsistent answers. By maintaining all intermediate steps and critiques, the agent ensures comprehensive and context-aware responses. This design choice is critical for multi-step, tool-dependent tasks where earlier results inform later reasoning. \n\n## Nodes\n\nThe Critic Agent's graph has two nodes, `assistant` and `tools`, which collaborate to iteratively answer the query.\n\n- **`assistant` Node**:\n  - **Input**: The initial question and the `Observation` (result of a tool invocation).\n  - **Processing**: \n    - The `assistant` generates a `Thought` by reasoning about whether the current information is sufficient to answer the question.\n    - If more information is needed, the `assistant` generates an `Action`, specifying a tool call and its arguments (e.g., search query for Tavily Search or Python code for Riza Code Interpreter).\n  - **Output**: A `Thought` (decision) and, if needed, an `Action` (tool call).\n\n- **`tools` Node**:\n  - **Input**: The `Action` generated by the `assistant`.\n  - **Execution**: Invokes the specified tool with the provided arguments and retrieves the result.\n  - **Output**: An `Observation`, which is passed back to the `assistant`.\n\n#### Example (TriviaQA):\nFor the question *“Which innovation for the car was developed by Prince Henry of Prussia in 1911?”*:\n\n1. The `assistant` node receives the query and decides to search for information about Henry of Prussia, generating an `Action` (search query).\n2. The `tools` node executes the search and returns the result (`Observation`) to the `assistant`.\n3. The `assistant` evaluates the observation, determines that more information is needed, and generates another `Action` to search for innovations by Prince Henry.\n4. The `tools` node performs the second search, and the result allows the `assistant` to conclude the answer.\n\nThis iterative process continues until the `assistant` determines the answer or a termination condition is met (e.g., maximum iterations).\n\n#### Interaction and Graph Dynamics:\n\nThe Critic Agent operates as a state machine:\n\n- **Nodes**: Functions that process and transform the agent's state.\n- **Edges**: Transitions between nodes based on the agent’s decisions (`Thought` and `Action`).\n\n#### State Attributes as Independent Channels\n\nIn LangGraph, each attribute of the state, such as `p`, `x`, `c`, and `y`, functions as an independent **channel** that can be updated individually. \n\n- **Partial Updates**: Functions operating on the state (nodes) do not need to return a complete state object. Instead, they can selectively update one or more attributes (channels) while leaving the others unchanged.\n- **State Compilation**: During the graph compilation process, LangGraph ensures that all node functions output a complete state object. It merges the unchanged attributes with the updated ones, creating a seamless transition between nodes.\n- **Example**: In the `assistant` node, the function only updates the `y` attribute of the state. Similarly, the `tools` node updates `c` and `num_iterations`. LangGraph handles the integration of these updates into the full state object.\n\nThis feature simplifies node implementation and enhances modularity, as each node can focus solely on the attributes it directly affects. LangGraph ensures the consistency and integrity of the full state during execution. \n\n## Conditional Edges\n\nThe `tool_call` function defines a conditional edge in the Critic Agent's graph. This edge determines whether the agent invokes a tool for additional observations or concludes its execution. \n\n- **Conditions**:\n  1. **No Pending Tool Calls**:\n     - If the most recent output (`state.y[-1]`) does not contain any tool calls, the agent assumes the assistant node has provided a satisfactory result validated by the Critic.\n     - The agent transitions to the `END` node, returning the result as the final output.\n  2. **Maximum Iterations Reached**:\n     - If the number of iterations (`state.num_iterations`) equals or exceeds the predefined limit (`MAX_NUM_ITERATIONS`), the agent forcefully terminates its process to prevent infinite loops.\n\n- **Code Logic**:\n  ```python\n  return END if (not state.y[-1].tool_calls) or state.num_iterations >= MAX_NUM_ITERATIONS else \"tools\"\n\n## Build the Graph\n\nLangGraph models applications as **state machines**, where the system transitions between defined states based on inputs and conditions. A state machine is a mathematical abstraction used to represent a system with a finite number of states, transitions, and actions. For more details, refer to the [Wikipedia article on finite-state machines](https://en.wikipedia.org/wiki/Finite-state_machine).\n\nThis cell defines the graph for the Critic Agent, representing its workflow as a state machine.\n\n#### Key Components:\n\n1. **State Class**:\n   - The `CriticState` class defines the state of the agent, storing attributes such as the system prompt (`p`), input (`x`), message history (`y`), tool responses (`c`), and the number of iterations (`num_iterations`).\n\n2. **Nodes**:\n   - Nodes are **functions mapping a state onto a new state**. Each node takes a `CriticState` as input and returns an updated `CriticState` after performing a specific operation:\n     - **`assistant`**: Processes the current state to generate a `Thought` (decision) and an `Action` (e.g., a tool call).\n     - **`tools`**: Executes the specified tool action and updates the state with the resulting observation.\n\n3. **Edges**:\n   - Edges are **functions on states** mapping them onto nodes. They dictate the control flow between them:\n     - **`START → assistant`**: The graph starts at the `assistant` node.\n     - **`assistant → tools` (Conditional Edge)**:\n       - The `tool_call` function determines whether to transition to the `tools` node or terminate the process based on whether additional tool calls are needed or the iteration limit is reached.\n     - **`tools → assistant`**: After executing a tool action, control flows back to the `assistant` node for further reasoning.\n\n4. **Graph Compilation**:\n   - The graph is constructed using LangGraph's `StateGraph` and compiled with `builder.compile()` to produce the executable state machine (`critic`).\n\n#### Workflow:\nThe Critic Agent alternates between reasoning (`assistant`) and tool execution (`tools`) until:\n\n1. It produces a satisfactory result validated by the Critic, or\n2. It reaches the maximum iteration limit (`MAX_NUM_ITERATIONS`).\n\nThis design ensures iterative improvement of the response while enforcing efficient termination when appropriate.\n\n## Run the Agent\n\nWe evaluate the model on 5 examples each from GSM8K[@cobbe2021gsm8k] and AmbigNQ[@min2020ambigqa] [@kwiatkowski2019natural]. The [GSM8K](https://github.com/openai/grade-school-math?utm_source=chatgpt.com) (Grade School Math 8K) dataset is a collection of 8.5K high-quality, linguistically diverse grade school math word problems. It is designed to evaluate the arithmetic reasoning capabilities of large language models (LLMs). The [AmbigNQ](https://github.com/shmsw25/AmbigQA) dataset is a collection of 14k questions derived from the NQ-open benchmark, specifically designed to explore ambiguity in open-domain question answering. For further details, refer to the [data utilities documentation](./utils/01_data.html).\n\n### Define Configuration\n\nWhen executing our agent, we can specify runtime parameters using the `config` argument in the `invoke` or `batch` methods. This argument accepts a [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) instance to control the agent’s behavior. In this example, we set a recursion limit of 5 to prevent the agent from calling the same node more than five times. Additionally, we limit the agent to a maximum of five concurrent calls to the language model (LLM) during batch operations. While the recursion limit serves to cap the number of iterations, it’s important to note that exceeding this limit will cause the agent to exit with an error, rather than completing its execution.\n\n### GSM8K\n\n#### Observations from the Output\n\n1. The CRITIC agent successfully translated mathematical problems into Python code and executed them using the Riza Code Interpreter tool.  \n2. The implementation achieved success on the first attempt, with no need for code revisions or re-writing.  \n\n### AmbigQA\n\n#### Observations from the Output\n\n1. The CRITIC agent successfully translated QA problems into search requests handled with TavilySearch.\n2. The implementation achieved success on the first attempt, with no need for further searches.  \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"00_critic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"lux","title-block-banner":true,"title":"The CRITIC","description":"This blog evaluates a CRITIC agent implemented using the LangGraph SDK, focusing on solving arithmetic and ambiguous questions. It utilizes the GSM8K and AmbigQA datasets to test mathematical reasoning and ambiguity resolution, respectively. The agent integrates computational and search tools for evaluating the correctness of the LLM generated answers.","author":"Oliver Pfante","date":"2024-11-23","categories":["agentic","code"],"image":"critic.webp","jupyter":"python3","bibliography":["../references.bibtex"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Agent Coder\"\n",
        "description: \"This blog explores the AgentCoder, a multi-agent framework for code generation which integrates. We build it in LangGraph, a Python SDK designed for creating LLM agents capable of reliably managing complex tasks.\"\n",
        "author: \"Oliver Pfante\"\n",
        "date: \"2025-01-02\"\n",
        "categories: [agentic, code]\n",
        "image: \"agent_coder.webp\"\n",
        "toc: true\n",
        "jupyter: python3\n",
        "execute: \n",
        "  enabled: true\n",
        "bibliography: ../references.bibtex\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "AgentCoder is a multi-agent framework designed to enhance code generation by leveraging three distinct roles to streamline the coding and validation process. This implementation is directly derived from the paper *\"AgentCoder: Multi-Agent Code Generation with Effective Testing and Self-Optimization\"*[@huang2024agentcodermultiagentbasedcodegeneration], with some minor modifications to the prompts to optimize its functionality within LangGraph.\n",
        "\n",
        "### **Key Features of AgentCoder**\n",
        "1. **Three Roles**: \n",
        "   - **Programmer**: Responsible for generating code based on requirements.\n",
        "   - **Test Designer**: Generates comprehensive, independent test cases without being influenced by the program code. The test cases cover basic ones, edge cases, and large scale test cases, testing for performance.\n",
        "   - **Test Executor**: A program (not an LLM persona) that runs the tests and provides feedback for further code refinement.\n",
        "2. **Independent Test Creation**: Ensures objectivity and enhances the reliability of generated tests.\n",
        "3. **Iterative Improvement**: Refines code through multiple iterations based on test feedback until all tests pass.\n",
        "\n",
        "### **Blog Objectives**\n",
        "- Illustrate the implementation of the AgentCoder framework using LangGraph.\n",
        "- Showcase the use of **Checkpointer (Memory)** in LangGraph to access the state of an executed graph.\n",
        "- Direct derivation of the implementation from the research paper, with minor modifications to the prompts.\n",
        "\n",
        "## Setup\n"
      ],
      "id": "d7368a3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langgraph.graph import StateGraph, END, START, MessagesState\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    PromptTemplate,\n",
        ")\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Annotated, Literal, TypedDict\n",
        "from operator import add, attrgetter\n",
        "import textwrap\n",
        "import os\n",
        "from trustcall import create_extractor"
      ],
      "id": "e23d9e3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts\n",
        "\n",
        "We start with writing down the relevant prompts. They are copy-pasted from the appendix of the paper.\n"
      ],
      "id": "176b79da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# first example from the HumanEval dataset. Used as one-shot example.\n",
        "\n",
        "human_eval_example = {\n",
        "    \"code_snippet\": 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
        "    \"entry_point\": \"has_close_elements\",\n",
        "}"
      ],
      "id": "b898294f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Programmer Prompt\n",
        "coder_prompt = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    **Role**: You are a software programmer. \n",
        "    \n",
        "    **Task**: As a programmer, you are required to complete the function. Use a Chain-of-Thought approach to break down the problem, create pseudocode, and then write the code in Python language. Ensure that your code is efficient, readable, and well-commented. For example: \n",
        "    \n",
        "    **Input Code Snippet**: \n",
        "    ```python \n",
        "    {code_snippet}\n",
        "    # TODO: Implement the logic to determine if any two numbers are closer than the threshold pass \n",
        "\n",
        "    # Add your code here to complete the function\n",
        "    ```\n",
        "\n",
        "    **Instructions**: \n",
        "    1. **Understand and Clarify**: Make sure you understand the task. \n",
        "    2. **Algorithm/Method Selection**: Decide on the most efficient way. \n",
        "    3. **Pseudocode Creation**: Write down the steps you will follow in pseudocode. \n",
        "    4. **Code Generation**: Translate your pseudocode into executable Python code.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "# one-shot example\n",
        "response = textwrap.dedent(\n",
        "    '''\n",
        "    As a software developer, I'll follow the Chain-of-Thought approach outlined in the prompt to complete the `has_close_elements` function. Let's break it down into the four steps: \n",
        "    \n",
        "    1. **Understand and Clarify**: \n",
        "    - The task is to write a function that checks if any two numbers in a given list are closer to each other than a specified threshold. \n",
        "    - The function should return `True` if such a pair exists, otherwise `False`. \n",
        "    \n",
        "    2. **Algorithm/Method Selection**: \n",
        "    - A straightforward way to solve this would be to compare each number with every other number in the list. \n",
        "    - We can use a nested loop where each number is compared to every number that comes after it in the list. \n",
        "    - The absolute difference between each pair will be calculated and compared to the threshold. \n",
        "    \n",
        "    3. **Pseudocode Creation**: \n",
        "    ```\n",
        "    function has_close_elements(numbers, threshold): \n",
        "        loop through each number in the list (i) \n",
        "            for each number, loop through the remaining numbers in the list (j) \n",
        "                if the absolute difference between numbers[i] and numbers[j] is less than or equal to the threshold \n",
        "                    return True \n",
        "    if no such pair is found, return False \n",
        "    ``` \n",
        "    \n",
        "    4. **Code Generation**: \n",
        "    - Now, I'll translate the pseudocode into Python code. Do not add any test cases in the implementation. Here's the \n",
        "      complete function: \n",
        "    ```python \n",
        "    from typing import List \n",
        "    \n",
        "    def has_close_elements(numbers: List[float], threshold: float) -> bool: \n",
        "        \"\"\" \n",
        "        Check if in given list of numbers, are any two numbers closer to each other than given threshold. \n",
        "        \n",
        "        >>> has_close_elements([1.0, 2.0, 3.0], 0.5) \n",
        "        False \n",
        "        >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) \n",
        "        True \n",
        "        \"\"\" \n",
        "        # Loop through each number in the list \n",
        "        for i in range(len(numbers)): \n",
        "            # For each number, loop through the remaining numbers \n",
        "            for j in range(i + 1, len(numbers)): \n",
        "                # If the absolute difference is less than or equal to the threshold, return True \n",
        "                if abs(numbers[i] - numbers[j]) <= threshold: \n",
        "                    return True \n",
        "                    \n",
        "        # If no such pair is found, return False \n",
        "        return False \n",
        "        ```\n",
        "'''\n",
        ").strip()\n",
        "\n",
        "# Prompt to structure the output generated by the Programmer\n",
        "structure_code = textwrap.dedent(\n",
        "    '''\n",
        "    Extract or update the items of the 'Coder' Pydantic class from the previous conversation:\n",
        "                                    \n",
        "    <convo>\n",
        "    {conversation}\n",
        "    </convo>\"\"\"\n",
        "    '''\n",
        ").strip()"
      ],
      "id": "de075be2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test Designer Prompt\n",
        "tester_prompt = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    **Role**: As a tester, your task is to create comprehensive test cases for the incomplete `{entry_point}` function. These test cases should encompass Basic, Edge, and Large Scale scenarios to ensure the code's robustness, reliability, and scalability. \n",
        "    \n",
        "    **Input Code Snippet**: \n",
        "    ```python\n",
        "    {code_snippet} \n",
        "    ```\n",
        "    **1. Basic Test Cases**: \n",
        "    - **Objective**: To verify the fundamental functionality of the `{entry_point}` function under normal conditions. \n",
        "    \n",
        "    **2. Edge Test Cases**: \n",
        "    - **Objective**: To evaluate the function's behavior under extreme or unusual conditions. \n",
        "    \n",
        "    **3. Large Scale Test Cases**: \n",
        "    - **Objective**: To assess the function’s performance and scalability with large data samples. \n",
        "    \n",
        "    **Instructions**: \n",
        "    - Implement a comprehensive set of test cases following the guidelines above. \n",
        "    - Ensure each test case is well-documented with comments explaining the scenario it covers. \n",
        "    - Pay special attention to edge cases as they often reveal hidden bugs. \n",
        "    - For large-scale tests, focus on the function's efficiency and performance under heavy loads.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "# one-shot example\n",
        "tester_response = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    **Role**: As a tester, your task is to create and execute a series of test cases for the `{entry_point}` function. These test cases should include Basic, Edge, and Large Scale scenarios to ensure the function's robustness, reliability, and scalability.\n",
        "\n",
        "    **Input Code Snippet**:\n",
        "    ```python\n",
        "    {code_snippet}\n",
        "    ```\n",
        "\n",
        "    **1. Basic Test Cases**:\n",
        "    - **Objective**: Verify the fundamental functionality of the `{entry_point}` function under normal conditions.\n",
        "    - **Examples**:\n",
        "        ```python\n",
        "        # Test 1: Standard case with close elements\n",
        "        assert (x := {entry_point}([1.0, 2.5, 3.5, 5.0], 1.0)) == True, str(x) + ' differs from the expected output'\n",
        "        \n",
        "        # Test 2: Standard case with no close elements\n",
        "        assert (x := {entry_point}([1.0, 3.0, 5.0, 7.0], 1.5)) == False, str(x) + ' differs from the expected output'\n",
        "        ```\n",
        "\n",
        "    **2. Edge Test Cases**:\n",
        "    - **Objective**: Evaluate the function's behavior under extreme or unusual conditions.\n",
        "    - **Examples**:\n",
        "        ```python\n",
        "        # Test 1: Empty list\n",
        "        assert (x := {entry_point}([], 1.0)) == False, str(x) + ' differs from the expected output'\n",
        "        \n",
        "        # Test 2: List with all identical elements\n",
        "        assert (x := {entry_point}([3.0, 3.0, 3.0], 0.0)) == True, str(x) + ' differs from the expected output'\n",
        "        \n",
        "        # Test 3: Very small threshold\n",
        "        assert (x := {entry_point}([1.0, 1.01, 2.0], 0.005)) == False, str(x) + ' differs from the expected output'\n",
        "        \n",
        "        # Test 4: List with only two elements\n",
        "        assert (x := {entry_point}([1.0, 2.0], 1.5)) == True, str(x) + ' differs from the expected output'\n",
        "        ```\n",
        "\n",
        "    **3. Large Scale Test Cases**:\n",
        "    - **Objective**: Assess the function’s performance and scalability with large data samples.\n",
        "    - **Examples**:\n",
        "        ```python\n",
        "        # Test 1: Large list with predictable pattern\n",
        "        large_list = [i * 0.1 for i in range(100000)]  # Creates a list [0, 0.1, 0.2, ..., 9999.9]\n",
        "        \n",
        "        # Test with a threshold where no close elements exist\n",
        "        assert (x := {entry_point}(large_list, 0.05)) == False, str(x) + ' differs from the expected output'\n",
        "        \n",
        "        # Test with a larger threshold where adjacent elements are within the threshold\n",
        "        assert (x := {entry_point}(large_list, 0.15)) == True, str(x) + ' differs from the expected output'\n",
        "        ```\n",
        "\n",
        "    **Instructions**:\n",
        "    - Implement and execute these test cases.\n",
        "    - Document any errors, inefficiencies, or unexpected behaviors observed during testing.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "# Prompt to structure the output of the Test Designer\n",
        "structure_test = textwrap.dedent(\n",
        "    '''\n",
        "    Extract the items of the 'Tester' Pydantic class from the previous conversation:\n",
        "                                    \n",
        "    <convo>\n",
        "    {conversation}\n",
        "    </convo>\"\"\"\n",
        "    '''\n",
        ").strip()"
      ],
      "id": "e30d4fc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Templates\n",
        "\n",
        "We require the output of the Programmer and the Test Designer to be structured. To achieve this, we define a Pydantic class for each."
      ],
      "id": "14292e7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Coder(BaseModel):\n",
        "    \"\"\"    \n",
        "    Pydantic class where each attribute corresponds to the step the Programmer should\n",
        "    follow to solve the coding challenge.\n",
        "    \"\"\"\n",
        "    clarify: str = Field(description=\"Make sure you understand the task.\")\n",
        "    algo: str = Field(\n",
        "        description=\"Decide on the most efficient implementation.\")\n",
        "    pseudocode: str = Field(\n",
        "        description=\"Write down the steps you follow in the implementation in pseudocode.\"\n",
        "    )\n",
        "    code: str = Field(\n",
        "        description=\"translate your pseudocode into executable Python code\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Tester(BaseModel):\n",
        "    \"\"\"\n",
        "    Pydantic class collecting the basic, edge cases, and large scale test cases.\n",
        "    \"\"\"\n",
        "    basic: List[str] = Field(\n",
        "        description=\"Basic test cases, each of them in Python code\"\n",
        "    )\n",
        "    edge: List[str] = Field(\n",
        "        description=\"Edge case test cases, each of them in Python code\"\n",
        "    )\n",
        "    large: List[str] = Field(\n",
        "        description=\"Large scale test cases, testing for performance, each of the in Python code\"\n",
        "    )"
      ],
      "id": "232166cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Runnables\n",
        "\n",
        "The LangChain [Runnable interface](https://python.langchain.com/docs/concepts/runnables/) provides a flexible and composable abstraction for building and chaining operations, including models, prompts, and tools. The Runnables will then be executed in the LangGraph nodes.\n"
      ],
      "id": "b1170043"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup Large Language Model (LLM)\n",
        "LLM = ChatOpenAI(\n",
        "    model_name=\"gpt-4o\",\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=0.0,\n",
        ")"
      ],
      "id": "3a1c0747",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Programmer’s prompt template includes both the instruction and a one-shot example. Later, the prompt will include the chat-history as context. Additionally, we crate a [Trustcall Runnable](https://github.com/hinthornw/trustcall) `code`. Trustcall is a Python package that enhances the generation of JSON schemas from LLM responses. Since we anticipate the LLM will iteratively refine the generated code based on feedback from test executions, we define an [Update Schema](https://github.com/hinthornw/trustcall?tab=readme-ov-file#updating-schemas) by enable insertions. \n"
      ],
      "id": "4cee6d3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coder_template = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        (\"user\", coder_prompt.format(**human_eval_example)),\n",
        "        (\"ai\", response),\n",
        "        (\"user\", coder_prompt),\n",
        "        MessagesPlaceholder(\"messages\", optional=True),\n",
        "    ],\n",
        "    input_variables=[\"code_snippet\", \"messages\"],\n",
        ")\n",
        "\n",
        "# Trustcall object for structuring the Programmer's output\n",
        "code = create_extractor(\n",
        "    LLM, tools=[Coder], tool_choice=\"Coder\", enable_inserts=True)"
      ],
      "id": "93ae5cfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tester's template including the instruction and a one-shot example.\n",
        "tester_template = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        (\"user\", tester_prompt.format(**human_eval_example)),\n",
        "        (\"ai\", tester_response.format(**human_eval_example)),\n",
        "        (\"user\", tester_prompt),\n",
        "    ],\n",
        "    input_variables=[\"code_snippet\", \"entry_point\"],\n",
        ")\n",
        "\n",
        "# Trustcall object for structuring the Tester's output\n",
        "tests = create_extractor(LLM, tools=[Tester], tool_choice=\"Tester\")"
      ],
      "id": "12455e39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## States\n",
        "\n",
        "We define a separate Input state to keep the unchanged input separated from the state channels which will be updated: the test cases, and the code.\n"
      ],
      "id": "d1788c82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class InputState(TypedDict):\n",
        "    code_snippet: str  # function header including its doc-string, i.e., the input from HumanEval\n",
        "    entry_point: str  # name of the function\n",
        "\n",
        "\n",
        "class OverallState(InputState, MessagesState):\n",
        "    # Since we inherit from Message state, we have also a 'messages' channel storing the chat history\n",
        "    tester: Tester  # test cases, basic, edge cases, and large scale test cases\n",
        "    # accumulates all implementation attempts\n",
        "    coder: Annotated[List[Coder], add]"
      ],
      "id": "5332df1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nodes\n",
        "\n",
        "The nodes of the Graph invoke the previously defined Runnables.\n"
      ],
      "id": "ff3d3e33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def write_program(state: OverallState) -> OverallState:\n",
        "    \"\"\"\n",
        "    Writes the program as string. Represents the 'Programmer'.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the program, which includes \n",
        "            the input context and the current implementation (if it exists)\n",
        "\n",
        "    Returns:\n",
        "        The updated state with new messages containing the generated program.\n",
        "    \"\"\"\n",
        "    return {\"messages\": (coder_template | LLM).invoke(state)}\n",
        "\n",
        "\n",
        "def get_code(state: OverallState) -> OverallState:\n",
        "    \"\"\"\n",
        "    Extracts the steps, the Programmer followed to solve the coding challenge as JSON schema\n",
        "    using a Trustcall runnable. It does it, by updating an already existing schema if it exists.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the program, containing the conversation \n",
        "            history and the coder's existing implementation as string.\n",
        "\n",
        "    Returns:\n",
        "        The updated state with the structured output containing the code in the \"coder\" key.\n",
        "    \"\"\"\n",
        "    if state[\"coder\"]:\n",
        "        structured = code.invoke(\n",
        "            {\n",
        "                \"messages\": [\n",
        "                    (\n",
        "                        \"user\",\n",
        "                        structure_code.format(\n",
        "                            conversation=state[\"messages\"][-1].content\n",
        "                        ),\n",
        "                    )\n",
        "                ],\n",
        "                \"existing\": {\"Coder\": state[\"coder\"][-1].model_dump()},\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        structured = code.invoke(\n",
        "            structure_code.format(conversation=state[\"messages\"][-1].content)\n",
        "        )\n",
        "    return {\"coder\": structured[\"responses\"]}\n",
        "\n",
        "\n",
        "def write_tests(state: OverallState) -> OverallState:\n",
        "    \"\"\"\n",
        "    Generates test cases for the provided state using the defined templates and parsers.\n",
        "    Represents the 'Test Designer'\n",
        "\n",
        "    Args:\n",
        "        state: The current state containing the context and requirements for generating tests.\n",
        "\n",
        "    Returns:\n",
        "        An updated state with the generated test cases stored under the \"tester\" key.\n",
        "    \"\"\"\n",
        "    structured_tests = (\n",
        "        tester_template\n",
        "        | LLM\n",
        "        | StrOutputParser()\n",
        "        | PromptTemplate.from_template(structure_test)\n",
        "        | tests\n",
        "    ).invoke(state)\n",
        "    return {\"tester\": structured_tests[\"responses\"][0]}\n",
        "\n",
        "\n",
        "def executor(state: OverallState) -> OverallState:\n",
        "    \"\"\"\n",
        "    Executes the generated code against the provided test cases and captures error messages.\n",
        "    Appends them to the message history. This node represents the 'Executor'.\n",
        "\n",
        "    Args:\n",
        "        state: The current state containing the code and test cases.\n",
        "\n",
        "    Returns:\n",
        "        Updated state with error messages for failed tests, if any.\n",
        "    \"\"\"\n",
        "    error_messages = []\n",
        "    for test_type in [\"basic\", \"edge\", \"large\"]:\n",
        "        for test in attrgetter(test_type)(state[\"tester\"]):\n",
        "            try:\n",
        "                exec(state[\"coder\"][-1].code + test)\n",
        "            except Exception as e:\n",
        "                error_messages.append(\n",
        "                    (\n",
        "                        \"user\",\n",
        "                        textwrap.dedent(\n",
        "                            \"\"\"\n",
        "                            FAILED TEST\n",
        "\n",
        "                            Your solution failed the test \n",
        "                                    \n",
        "                            <test>               \n",
        "                            {test}\n",
        "                            <test>\n",
        "\n",
        "                            with the error message:\n",
        "\n",
        "                            <error>\n",
        "                            {error}\n",
        "                            <error>                \n",
        "                            \"\"\"\n",
        "                        )\n",
        "                        .strip()\n",
        "                        .format(test=test, error=e),\n",
        "                    )\n",
        "                )\n",
        "    return {\"messages\": error_messages}"
      ],
      "id": "1a6e99b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional Edges\n"
      ],
      "id": "44d5cae1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def correct_implementation(state: OverallState) -> Literal[\"Programmer\", \"END\"]:\n",
        "    \"\"\"\n",
        "    Decides the next workflow step based on test results.\n",
        "\n",
        "    If any test fails, the workflow returns to \"Programmer\" for refinement. Otherwise, it proceeds to \"END\".\n",
        "\n",
        "    Args:\n",
        "        state: The current workflow state with messages and test results.\n",
        "\n",
        "    Returns:\n",
        "        \"Programmer\" if tests failed, or \"END\" if all tests passed.\n",
        "    \"\"\"\n",
        "    if \"FAILED TEST\" in state[\"messages\"][-1].content:\n",
        "        return \"Programmer\"\n",
        "    else:\n",
        "        return \"END\""
      ],
      "id": "63765216",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Graph\n",
        "\n",
        "We add [Short-term Memory](https://langchain-ai.github.io/langgraph/concepts/memory/) to the Graph because we want to access the Graph's state after it has been executed. Recording the state updates by enabling check points also helps with debugging.\n"
      ],
      "id": "afa36ad5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workflow = StateGraph(OverallState, input=InputState)\n",
        "\n",
        "# add nodes\n",
        "workflow.add_node(\"Programmer\", write_program)\n",
        "workflow.add_node(\"Get Code\", get_code)\n",
        "workflow.add_node(\"Test Designer\", write_tests)\n",
        "workflow.add_node(\"Executor\", executor)\n",
        "\n",
        "# add edges\n",
        "workflow.add_edge(START, \"Programmer\")\n",
        "workflow.add_edge(START, \"Test Designer\")\n",
        "workflow.add_edge(\"Programmer\", \"Get Code\")\n",
        "workflow.add_edge(\"Get Code\", \"Executor\")\n",
        "workflow.add_edge(\"Test Designer\", \"Executor\")\n",
        "workflow.add_conditional_edges(\"Executor\", correct_implementation, {\n",
        "                               \"Programmer\": \"Programmer\", \"END\": END})\n",
        "\n",
        "# compile the graph\n",
        "\n",
        "memory = MemorySaver()\n",
        "coder = workflow.compile(checkpointer=memory)"
      ],
      "id": "91df5751",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(coder.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "id": "fbdab3bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Agent\n",
        "\n",
        "Execute the graph in streaming mode and render the messages. We use the ‘values’ streaming mode, which retains the entire chat history at each step. To focus on updates, only the most recent message is displayed. As input we pick an example of the [HumanEval](https://github.com/openai/human-eval)[@chen2021codex] test set.\n"
      ],
      "id": "f319aee5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One sample from the HumanEval testset\n",
        "problem = {\n",
        "    \"task_id\": \"HumanEval\\/1\",\n",
        "    \"prompt\": \"from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \\\"\\\"\\\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups('( ) (( )) (( )( ))')\\n    ['()', '(())', '(()())']\\n    \\\"\\\"\\\"\\n\",\n",
        "    \"canonical_solution\": \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
        "    \"test\": \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\",\n",
        "    \"entry_point\": \"separate_paren_groups\",\n",
        "}\n",
        "\n",
        "# Input\n",
        "initial_input = {\n",
        "    \"code_snippet\": problem[\"prompt\"],\n",
        "    \"entry_point\": problem[\"entry_point\"],\n",
        "}\n",
        "\n",
        "# Thread\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Run the agent by streaming the graph\n",
        "for event in coder.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    if event[\"messages\"]:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "id": "73d9bc34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last, we access the final implementation and run it against the HumanEval test cases. \n"
      ],
      "id": "270055ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Access the final state of the graph\n",
        "state = coder.get_state(thread)\n",
        "\n",
        "# Run tests on the final implementation\n",
        "exec(\n",
        "    state.values[\"coder\"][-1].code\n",
        "    + problem[\"test\"]\n",
        "    + f\"\\ncheck({problem['entry_point']})\"\n",
        ")\n",
        "\n",
        "print(f'Tests ran successful after {len(state.values[\"coder\"])} iterations!')"
      ],
      "id": "c9ea05ab",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/oliverpfante/Documents/agentic/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}